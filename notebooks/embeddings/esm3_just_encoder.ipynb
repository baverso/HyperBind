{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7a37bdc0-933a-4c94-9464-33d5397fef2d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.serialization import safe_globals\n",
    "\n",
    "from esm.models.esm3 import ESM3\n",
    "from esm.tokenization.sequence_tokenizer import EsmSequenceTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c2453503-2177-4a8f-a87c-084bcea668eb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Register the ESM3 class as safe for unpickling.\n",
    "torch.serialization.add_safe_globals([ESM3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "78a3d589-a88e-4fd2-9bdd-e6c5980cadd3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2229619655.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[7], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    https://github.com/evolutionaryscale/esm/issues/178\u001b[0m\n\u001b[0m          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "https://github.com/evolutionaryscale/esm/issues/178"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "679b7fa5-2f4a-4af3-9dd2-419b7236dca0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "backbone_save_path = \"/home/jupyter/DATA/evqlv-dev/model-weights/esm3_backbone/esm3_backbone_model.pt\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)\n",
    "with safe_globals([(\"esm.models.esm3.ESM3\", ESM3)]):\n",
    "    loaded_backbone_model = torch.load(backbone_save_path, map_location=device, weights_only=False)\n",
    "    loaded_backbone_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "202f3562-e069-46f6-a5e1-95b97ced0dfa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder.sequence_embed.weight: torch.Size([64, 1536])\n",
      "encoder.plddt_projection.weight: torch.Size([1536, 16])\n",
      "encoder.plddt_projection.bias: torch.Size([1536])\n",
      "encoder.structure_per_res_plddt_projection.weight: torch.Size([1536, 16])\n",
      "encoder.structure_per_res_plddt_projection.bias: torch.Size([1536])\n",
      "encoder.structure_tokens_embed.weight: torch.Size([4101, 1536])\n",
      "encoder.ss8_embed.weight: torch.Size([11, 1536])\n",
      "encoder.sasa_embed.weight: torch.Size([19, 1536])\n",
      "encoder.function_embed.0.weight: torch.Size([260, 192])\n",
      "encoder.function_embed.1.weight: torch.Size([260, 192])\n",
      "encoder.function_embed.2.weight: torch.Size([260, 192])\n",
      "encoder.function_embed.3.weight: torch.Size([260, 192])\n",
      "encoder.function_embed.4.weight: torch.Size([260, 192])\n",
      "encoder.function_embed.5.weight: torch.Size([260, 192])\n",
      "encoder.function_embed.6.weight: torch.Size([260, 192])\n",
      "encoder.function_embed.7.weight: torch.Size([260, 192])\n",
      "encoder.residue_embed.weight: torch.Size([1478, 1536])\n",
      "transformer.blocks.0.attn.layernorm_qkv.0.weight: torch.Size([1536])\n",
      "transformer.blocks.0.attn.layernorm_qkv.0.bias: torch.Size([1536])\n",
      "transformer.blocks.0.attn.layernorm_qkv.1.weight: torch.Size([4608, 1536])\n",
      "transformer.blocks.0.attn.out_proj.weight: torch.Size([1536, 1536])\n",
      "transformer.blocks.0.attn.q_ln.weight: torch.Size([1536])\n",
      "transformer.blocks.0.attn.k_ln.weight: torch.Size([1536])\n",
      "transformer.blocks.0.geom_attn.distance_scale_per_head: torch.Size([256])\n",
      "transformer.blocks.0.geom_attn.rotation_scale_per_head: torch.Size([256])\n",
      "transformer.blocks.0.geom_attn.s_norm.weight: torch.Size([1536])\n",
      "transformer.blocks.0.geom_attn.proj.weight: torch.Size([3840, 1536])\n",
      "transformer.blocks.0.geom_attn.out_proj.weight: torch.Size([1536, 768])\n",
      "transformer.blocks.0.ffn.0.weight: torch.Size([1536])\n",
      "transformer.blocks.0.ffn.0.bias: torch.Size([1536])\n",
      "transformer.blocks.0.ffn.1.weight: torch.Size([8192, 1536])\n",
      "transformer.blocks.0.ffn.3.weight: torch.Size([1536, 4096])\n",
      "transformer.blocks.1.attn.layernorm_qkv.0.weight: torch.Size([1536])\n",
      "transformer.blocks.1.attn.layernorm_qkv.0.bias: torch.Size([1536])\n",
      "transformer.blocks.1.attn.layernorm_qkv.1.weight: torch.Size([4608, 1536])\n",
      "transformer.blocks.1.attn.out_proj.weight: torch.Size([1536, 1536])\n",
      "transformer.blocks.1.attn.q_ln.weight: torch.Size([1536])\n",
      "transformer.blocks.1.attn.k_ln.weight: torch.Size([1536])\n",
      "transformer.blocks.1.ffn.0.weight: torch.Size([1536])\n",
      "transformer.blocks.1.ffn.0.bias: torch.Size([1536])\n",
      "transformer.blocks.1.ffn.1.weight: torch.Size([8192, 1536])\n",
      "transformer.blocks.1.ffn.3.weight: torch.Size([1536, 4096])\n",
      "transformer.blocks.2.attn.layernorm_qkv.0.weight: torch.Size([1536])\n",
      "transformer.blocks.2.attn.layernorm_qkv.0.bias: torch.Size([1536])\n",
      "transformer.blocks.2.attn.layernorm_qkv.1.weight: torch.Size([4608, 1536])\n",
      "transformer.blocks.2.attn.out_proj.weight: torch.Size([1536, 1536])\n",
      "transformer.blocks.2.attn.q_ln.weight: torch.Size([1536])\n",
      "transformer.blocks.2.attn.k_ln.weight: torch.Size([1536])\n",
      "transformer.blocks.2.ffn.0.weight: torch.Size([1536])\n",
      "transformer.blocks.2.ffn.0.bias: torch.Size([1536])\n",
      "transformer.blocks.2.ffn.1.weight: torch.Size([8192, 1536])\n",
      "transformer.blocks.2.ffn.3.weight: torch.Size([1536, 4096])\n",
      "transformer.blocks.3.attn.layernorm_qkv.0.weight: torch.Size([1536])\n",
      "transformer.blocks.3.attn.layernorm_qkv.0.bias: torch.Size([1536])\n",
      "transformer.blocks.3.attn.layernorm_qkv.1.weight: torch.Size([4608, 1536])\n",
      "transformer.blocks.3.attn.out_proj.weight: torch.Size([1536, 1536])\n",
      "transformer.blocks.3.attn.q_ln.weight: torch.Size([1536])\n",
      "transformer.blocks.3.attn.k_ln.weight: torch.Size([1536])\n",
      "transformer.blocks.3.ffn.0.weight: torch.Size([1536])\n",
      "transformer.blocks.3.ffn.0.bias: torch.Size([1536])\n",
      "transformer.blocks.3.ffn.1.weight: torch.Size([8192, 1536])\n",
      "transformer.blocks.3.ffn.3.weight: torch.Size([1536, 4096])\n",
      "transformer.blocks.4.attn.layernorm_qkv.0.weight: torch.Size([1536])\n",
      "transformer.blocks.4.attn.layernorm_qkv.0.bias: torch.Size([1536])\n",
      "transformer.blocks.4.attn.layernorm_qkv.1.weight: torch.Size([4608, 1536])\n",
      "transformer.blocks.4.attn.out_proj.weight: torch.Size([1536, 1536])\n",
      "transformer.blocks.4.attn.q_ln.weight: torch.Size([1536])\n",
      "transformer.blocks.4.attn.k_ln.weight: torch.Size([1536])\n",
      "transformer.blocks.4.ffn.0.weight: torch.Size([1536])\n",
      "transformer.blocks.4.ffn.0.bias: torch.Size([1536])\n",
      "transformer.blocks.4.ffn.1.weight: torch.Size([8192, 1536])\n",
      "transformer.blocks.4.ffn.3.weight: torch.Size([1536, 4096])\n",
      "transformer.blocks.5.attn.layernorm_qkv.0.weight: torch.Size([1536])\n",
      "transformer.blocks.5.attn.layernorm_qkv.0.bias: torch.Size([1536])\n",
      "transformer.blocks.5.attn.layernorm_qkv.1.weight: torch.Size([4608, 1536])\n",
      "transformer.blocks.5.attn.out_proj.weight: torch.Size([1536, 1536])\n",
      "transformer.blocks.5.attn.q_ln.weight: torch.Size([1536])\n",
      "transformer.blocks.5.attn.k_ln.weight: torch.Size([1536])\n",
      "transformer.blocks.5.ffn.0.weight: torch.Size([1536])\n",
      "transformer.blocks.5.ffn.0.bias: torch.Size([1536])\n",
      "transformer.blocks.5.ffn.1.weight: torch.Size([8192, 1536])\n",
      "transformer.blocks.5.ffn.3.weight: torch.Size([1536, 4096])\n",
      "transformer.blocks.6.attn.layernorm_qkv.0.weight: torch.Size([1536])\n",
      "transformer.blocks.6.attn.layernorm_qkv.0.bias: torch.Size([1536])\n",
      "transformer.blocks.6.attn.layernorm_qkv.1.weight: torch.Size([4608, 1536])\n",
      "transformer.blocks.6.attn.out_proj.weight: torch.Size([1536, 1536])\n",
      "transformer.blocks.6.attn.q_ln.weight: torch.Size([1536])\n",
      "transformer.blocks.6.attn.k_ln.weight: torch.Size([1536])\n",
      "transformer.blocks.6.ffn.0.weight: torch.Size([1536])\n",
      "transformer.blocks.6.ffn.0.bias: torch.Size([1536])\n",
      "transformer.blocks.6.ffn.1.weight: torch.Size([8192, 1536])\n",
      "transformer.blocks.6.ffn.3.weight: torch.Size([1536, 4096])\n",
      "transformer.blocks.7.attn.layernorm_qkv.0.weight: torch.Size([1536])\n",
      "transformer.blocks.7.attn.layernorm_qkv.0.bias: torch.Size([1536])\n",
      "transformer.blocks.7.attn.layernorm_qkv.1.weight: torch.Size([4608, 1536])\n",
      "transformer.blocks.7.attn.out_proj.weight: torch.Size([1536, 1536])\n",
      "transformer.blocks.7.attn.q_ln.weight: torch.Size([1536])\n",
      "transformer.blocks.7.attn.k_ln.weight: torch.Size([1536])\n",
      "transformer.blocks.7.ffn.0.weight: torch.Size([1536])\n",
      "transformer.blocks.7.ffn.0.bias: torch.Size([1536])\n",
      "transformer.blocks.7.ffn.1.weight: torch.Size([8192, 1536])\n",
      "transformer.blocks.7.ffn.3.weight: torch.Size([1536, 4096])\n",
      "transformer.blocks.8.attn.layernorm_qkv.0.weight: torch.Size([1536])\n",
      "transformer.blocks.8.attn.layernorm_qkv.0.bias: torch.Size([1536])\n",
      "transformer.blocks.8.attn.layernorm_qkv.1.weight: torch.Size([4608, 1536])\n",
      "transformer.blocks.8.attn.out_proj.weight: torch.Size([1536, 1536])\n",
      "transformer.blocks.8.attn.q_ln.weight: torch.Size([1536])\n",
      "transformer.blocks.8.attn.k_ln.weight: torch.Size([1536])\n",
      "transformer.blocks.8.ffn.0.weight: torch.Size([1536])\n",
      "transformer.blocks.8.ffn.0.bias: torch.Size([1536])\n",
      "transformer.blocks.8.ffn.1.weight: torch.Size([8192, 1536])\n",
      "transformer.blocks.8.ffn.3.weight: torch.Size([1536, 4096])\n",
      "transformer.blocks.9.attn.layernorm_qkv.0.weight: torch.Size([1536])\n",
      "transformer.blocks.9.attn.layernorm_qkv.0.bias: torch.Size([1536])\n",
      "transformer.blocks.9.attn.layernorm_qkv.1.weight: torch.Size([4608, 1536])\n",
      "transformer.blocks.9.attn.out_proj.weight: torch.Size([1536, 1536])\n",
      "transformer.blocks.9.attn.q_ln.weight: torch.Size([1536])\n",
      "transformer.blocks.9.attn.k_ln.weight: torch.Size([1536])\n",
      "transformer.blocks.9.ffn.0.weight: torch.Size([1536])\n",
      "transformer.blocks.9.ffn.0.bias: torch.Size([1536])\n",
      "transformer.blocks.9.ffn.1.weight: torch.Size([8192, 1536])\n",
      "transformer.blocks.9.ffn.3.weight: torch.Size([1536, 4096])\n",
      "transformer.blocks.10.attn.layernorm_qkv.0.weight: torch.Size([1536])\n",
      "transformer.blocks.10.attn.layernorm_qkv.0.bias: torch.Size([1536])\n",
      "transformer.blocks.10.attn.layernorm_qkv.1.weight: torch.Size([4608, 1536])\n",
      "transformer.blocks.10.attn.out_proj.weight: torch.Size([1536, 1536])\n",
      "transformer.blocks.10.attn.q_ln.weight: torch.Size([1536])\n",
      "transformer.blocks.10.attn.k_ln.weight: torch.Size([1536])\n",
      "transformer.blocks.10.ffn.0.weight: torch.Size([1536])\n",
      "transformer.blocks.10.ffn.0.bias: torch.Size([1536])\n",
      "transformer.blocks.10.ffn.1.weight: torch.Size([8192, 1536])\n",
      "transformer.blocks.10.ffn.3.weight: torch.Size([1536, 4096])\n",
      "transformer.blocks.11.attn.layernorm_qkv.0.weight: torch.Size([1536])\n",
      "transformer.blocks.11.attn.layernorm_qkv.0.bias: torch.Size([1536])\n",
      "transformer.blocks.11.attn.layernorm_qkv.1.weight: torch.Size([4608, 1536])\n",
      "transformer.blocks.11.attn.out_proj.weight: torch.Size([1536, 1536])\n",
      "transformer.blocks.11.attn.q_ln.weight: torch.Size([1536])\n",
      "transformer.blocks.11.attn.k_ln.weight: torch.Size([1536])\n",
      "transformer.blocks.11.ffn.0.weight: torch.Size([1536])\n",
      "transformer.blocks.11.ffn.0.bias: torch.Size([1536])\n",
      "transformer.blocks.11.ffn.1.weight: torch.Size([8192, 1536])\n",
      "transformer.blocks.11.ffn.3.weight: torch.Size([1536, 4096])\n",
      "transformer.blocks.12.attn.layernorm_qkv.0.weight: torch.Size([1536])\n",
      "transformer.blocks.12.attn.layernorm_qkv.0.bias: torch.Size([1536])\n",
      "transformer.blocks.12.attn.layernorm_qkv.1.weight: torch.Size([4608, 1536])\n",
      "transformer.blocks.12.attn.out_proj.weight: torch.Size([1536, 1536])\n",
      "transformer.blocks.12.attn.q_ln.weight: torch.Size([1536])\n",
      "transformer.blocks.12.attn.k_ln.weight: torch.Size([1536])\n",
      "transformer.blocks.12.ffn.0.weight: torch.Size([1536])\n",
      "transformer.blocks.12.ffn.0.bias: torch.Size([1536])\n",
      "transformer.blocks.12.ffn.1.weight: torch.Size([8192, 1536])\n",
      "transformer.blocks.12.ffn.3.weight: torch.Size([1536, 4096])\n",
      "transformer.blocks.13.attn.layernorm_qkv.0.weight: torch.Size([1536])\n",
      "transformer.blocks.13.attn.layernorm_qkv.0.bias: torch.Size([1536])\n",
      "transformer.blocks.13.attn.layernorm_qkv.1.weight: torch.Size([4608, 1536])\n",
      "transformer.blocks.13.attn.out_proj.weight: torch.Size([1536, 1536])\n",
      "transformer.blocks.13.attn.q_ln.weight: torch.Size([1536])\n",
      "transformer.blocks.13.attn.k_ln.weight: torch.Size([1536])\n",
      "transformer.blocks.13.ffn.0.weight: torch.Size([1536])\n",
      "transformer.blocks.13.ffn.0.bias: torch.Size([1536])\n",
      "transformer.blocks.13.ffn.1.weight: torch.Size([8192, 1536])\n",
      "transformer.blocks.13.ffn.3.weight: torch.Size([1536, 4096])\n",
      "transformer.blocks.14.attn.layernorm_qkv.0.weight: torch.Size([1536])\n",
      "transformer.blocks.14.attn.layernorm_qkv.0.bias: torch.Size([1536])\n",
      "transformer.blocks.14.attn.layernorm_qkv.1.weight: torch.Size([4608, 1536])\n",
      "transformer.blocks.14.attn.out_proj.weight: torch.Size([1536, 1536])\n",
      "transformer.blocks.14.attn.q_ln.weight: torch.Size([1536])\n",
      "transformer.blocks.14.attn.k_ln.weight: torch.Size([1536])\n",
      "transformer.blocks.14.ffn.0.weight: torch.Size([1536])\n",
      "transformer.blocks.14.ffn.0.bias: torch.Size([1536])\n",
      "transformer.blocks.14.ffn.1.weight: torch.Size([8192, 1536])\n",
      "transformer.blocks.14.ffn.3.weight: torch.Size([1536, 4096])\n",
      "transformer.blocks.15.attn.layernorm_qkv.0.weight: torch.Size([1536])\n",
      "transformer.blocks.15.attn.layernorm_qkv.0.bias: torch.Size([1536])\n",
      "transformer.blocks.15.attn.layernorm_qkv.1.weight: torch.Size([4608, 1536])\n",
      "transformer.blocks.15.attn.out_proj.weight: torch.Size([1536, 1536])\n",
      "transformer.blocks.15.attn.q_ln.weight: torch.Size([1536])\n",
      "transformer.blocks.15.attn.k_ln.weight: torch.Size([1536])\n",
      "transformer.blocks.15.ffn.0.weight: torch.Size([1536])\n",
      "transformer.blocks.15.ffn.0.bias: torch.Size([1536])\n",
      "transformer.blocks.15.ffn.1.weight: torch.Size([8192, 1536])\n",
      "transformer.blocks.15.ffn.3.weight: torch.Size([1536, 4096])\n",
      "transformer.blocks.16.attn.layernorm_qkv.0.weight: torch.Size([1536])\n",
      "transformer.blocks.16.attn.layernorm_qkv.0.bias: torch.Size([1536])\n",
      "transformer.blocks.16.attn.layernorm_qkv.1.weight: torch.Size([4608, 1536])\n",
      "transformer.blocks.16.attn.out_proj.weight: torch.Size([1536, 1536])\n",
      "transformer.blocks.16.attn.q_ln.weight: torch.Size([1536])\n",
      "transformer.blocks.16.attn.k_ln.weight: torch.Size([1536])\n",
      "transformer.blocks.16.ffn.0.weight: torch.Size([1536])\n",
      "transformer.blocks.16.ffn.0.bias: torch.Size([1536])\n",
      "transformer.blocks.16.ffn.1.weight: torch.Size([8192, 1536])\n",
      "transformer.blocks.16.ffn.3.weight: torch.Size([1536, 4096])\n",
      "transformer.blocks.17.attn.layernorm_qkv.0.weight: torch.Size([1536])\n",
      "transformer.blocks.17.attn.layernorm_qkv.0.bias: torch.Size([1536])\n",
      "transformer.blocks.17.attn.layernorm_qkv.1.weight: torch.Size([4608, 1536])\n",
      "transformer.blocks.17.attn.out_proj.weight: torch.Size([1536, 1536])\n",
      "transformer.blocks.17.attn.q_ln.weight: torch.Size([1536])\n",
      "transformer.blocks.17.attn.k_ln.weight: torch.Size([1536])\n",
      "transformer.blocks.17.ffn.0.weight: torch.Size([1536])\n",
      "transformer.blocks.17.ffn.0.bias: torch.Size([1536])\n",
      "transformer.blocks.17.ffn.1.weight: torch.Size([8192, 1536])\n",
      "transformer.blocks.17.ffn.3.weight: torch.Size([1536, 4096])\n",
      "transformer.blocks.18.attn.layernorm_qkv.0.weight: torch.Size([1536])\n",
      "transformer.blocks.18.attn.layernorm_qkv.0.bias: torch.Size([1536])\n",
      "transformer.blocks.18.attn.layernorm_qkv.1.weight: torch.Size([4608, 1536])\n",
      "transformer.blocks.18.attn.out_proj.weight: torch.Size([1536, 1536])\n",
      "transformer.blocks.18.attn.q_ln.weight: torch.Size([1536])\n",
      "transformer.blocks.18.attn.k_ln.weight: torch.Size([1536])\n",
      "transformer.blocks.18.ffn.0.weight: torch.Size([1536])\n",
      "transformer.blocks.18.ffn.0.bias: torch.Size([1536])\n",
      "transformer.blocks.18.ffn.1.weight: torch.Size([8192, 1536])\n",
      "transformer.blocks.18.ffn.3.weight: torch.Size([1536, 4096])\n",
      "transformer.blocks.19.attn.layernorm_qkv.0.weight: torch.Size([1536])\n",
      "transformer.blocks.19.attn.layernorm_qkv.0.bias: torch.Size([1536])\n",
      "transformer.blocks.19.attn.layernorm_qkv.1.weight: torch.Size([4608, 1536])\n",
      "transformer.blocks.19.attn.out_proj.weight: torch.Size([1536, 1536])\n",
      "transformer.blocks.19.attn.q_ln.weight: torch.Size([1536])\n",
      "transformer.blocks.19.attn.k_ln.weight: torch.Size([1536])\n",
      "transformer.blocks.19.ffn.0.weight: torch.Size([1536])\n",
      "transformer.blocks.19.ffn.0.bias: torch.Size([1536])\n",
      "transformer.blocks.19.ffn.1.weight: torch.Size([8192, 1536])\n",
      "transformer.blocks.19.ffn.3.weight: torch.Size([1536, 4096])\n",
      "transformer.blocks.20.attn.layernorm_qkv.0.weight: torch.Size([1536])\n",
      "transformer.blocks.20.attn.layernorm_qkv.0.bias: torch.Size([1536])\n",
      "transformer.blocks.20.attn.layernorm_qkv.1.weight: torch.Size([4608, 1536])\n",
      "transformer.blocks.20.attn.out_proj.weight: torch.Size([1536, 1536])\n",
      "transformer.blocks.20.attn.q_ln.weight: torch.Size([1536])\n",
      "transformer.blocks.20.attn.k_ln.weight: torch.Size([1536])\n",
      "transformer.blocks.20.ffn.0.weight: torch.Size([1536])\n",
      "transformer.blocks.20.ffn.0.bias: torch.Size([1536])\n",
      "transformer.blocks.20.ffn.1.weight: torch.Size([8192, 1536])\n",
      "transformer.blocks.20.ffn.3.weight: torch.Size([1536, 4096])\n",
      "transformer.blocks.21.attn.layernorm_qkv.0.weight: torch.Size([1536])\n",
      "transformer.blocks.21.attn.layernorm_qkv.0.bias: torch.Size([1536])\n",
      "transformer.blocks.21.attn.layernorm_qkv.1.weight: torch.Size([4608, 1536])\n",
      "transformer.blocks.21.attn.out_proj.weight: torch.Size([1536, 1536])\n",
      "transformer.blocks.21.attn.q_ln.weight: torch.Size([1536])\n",
      "transformer.blocks.21.attn.k_ln.weight: torch.Size([1536])\n",
      "transformer.blocks.21.ffn.0.weight: torch.Size([1536])\n",
      "transformer.blocks.21.ffn.0.bias: torch.Size([1536])\n",
      "transformer.blocks.21.ffn.1.weight: torch.Size([8192, 1536])\n",
      "transformer.blocks.21.ffn.3.weight: torch.Size([1536, 4096])\n",
      "transformer.blocks.22.attn.layernorm_qkv.0.weight: torch.Size([1536])\n",
      "transformer.blocks.22.attn.layernorm_qkv.0.bias: torch.Size([1536])\n",
      "transformer.blocks.22.attn.layernorm_qkv.1.weight: torch.Size([4608, 1536])\n",
      "transformer.blocks.22.attn.out_proj.weight: torch.Size([1536, 1536])\n",
      "transformer.blocks.22.attn.q_ln.weight: torch.Size([1536])\n",
      "transformer.blocks.22.attn.k_ln.weight: torch.Size([1536])\n",
      "transformer.blocks.22.ffn.0.weight: torch.Size([1536])\n",
      "transformer.blocks.22.ffn.0.bias: torch.Size([1536])\n",
      "transformer.blocks.22.ffn.1.weight: torch.Size([8192, 1536])\n",
      "transformer.blocks.22.ffn.3.weight: torch.Size([1536, 4096])\n",
      "transformer.blocks.23.attn.layernorm_qkv.0.weight: torch.Size([1536])\n",
      "transformer.blocks.23.attn.layernorm_qkv.0.bias: torch.Size([1536])\n",
      "transformer.blocks.23.attn.layernorm_qkv.1.weight: torch.Size([4608, 1536])\n",
      "transformer.blocks.23.attn.out_proj.weight: torch.Size([1536, 1536])\n",
      "transformer.blocks.23.attn.q_ln.weight: torch.Size([1536])\n",
      "transformer.blocks.23.attn.k_ln.weight: torch.Size([1536])\n",
      "transformer.blocks.23.ffn.0.weight: torch.Size([1536])\n",
      "transformer.blocks.23.ffn.0.bias: torch.Size([1536])\n",
      "transformer.blocks.23.ffn.1.weight: torch.Size([8192, 1536])\n",
      "transformer.blocks.23.ffn.3.weight: torch.Size([1536, 4096])\n",
      "transformer.blocks.24.attn.layernorm_qkv.0.weight: torch.Size([1536])\n",
      "transformer.blocks.24.attn.layernorm_qkv.0.bias: torch.Size([1536])\n",
      "transformer.blocks.24.attn.layernorm_qkv.1.weight: torch.Size([4608, 1536])\n",
      "transformer.blocks.24.attn.out_proj.weight: torch.Size([1536, 1536])\n",
      "transformer.blocks.24.attn.q_ln.weight: torch.Size([1536])\n",
      "transformer.blocks.24.attn.k_ln.weight: torch.Size([1536])\n",
      "transformer.blocks.24.ffn.0.weight: torch.Size([1536])\n",
      "transformer.blocks.24.ffn.0.bias: torch.Size([1536])\n",
      "transformer.blocks.24.ffn.1.weight: torch.Size([8192, 1536])\n",
      "transformer.blocks.24.ffn.3.weight: torch.Size([1536, 4096])\n",
      "transformer.blocks.25.attn.layernorm_qkv.0.weight: torch.Size([1536])\n",
      "transformer.blocks.25.attn.layernorm_qkv.0.bias: torch.Size([1536])\n",
      "transformer.blocks.25.attn.layernorm_qkv.1.weight: torch.Size([4608, 1536])\n",
      "transformer.blocks.25.attn.out_proj.weight: torch.Size([1536, 1536])\n",
      "transformer.blocks.25.attn.q_ln.weight: torch.Size([1536])\n",
      "transformer.blocks.25.attn.k_ln.weight: torch.Size([1536])\n",
      "transformer.blocks.25.ffn.0.weight: torch.Size([1536])\n",
      "transformer.blocks.25.ffn.0.bias: torch.Size([1536])\n",
      "transformer.blocks.25.ffn.1.weight: torch.Size([8192, 1536])\n",
      "transformer.blocks.25.ffn.3.weight: torch.Size([1536, 4096])\n",
      "transformer.blocks.26.attn.layernorm_qkv.0.weight: torch.Size([1536])\n",
      "transformer.blocks.26.attn.layernorm_qkv.0.bias: torch.Size([1536])\n",
      "transformer.blocks.26.attn.layernorm_qkv.1.weight: torch.Size([4608, 1536])\n",
      "transformer.blocks.26.attn.out_proj.weight: torch.Size([1536, 1536])\n",
      "transformer.blocks.26.attn.q_ln.weight: torch.Size([1536])\n",
      "transformer.blocks.26.attn.k_ln.weight: torch.Size([1536])\n",
      "transformer.blocks.26.ffn.0.weight: torch.Size([1536])\n",
      "transformer.blocks.26.ffn.0.bias: torch.Size([1536])\n",
      "transformer.blocks.26.ffn.1.weight: torch.Size([8192, 1536])\n",
      "transformer.blocks.26.ffn.3.weight: torch.Size([1536, 4096])\n",
      "transformer.blocks.27.attn.layernorm_qkv.0.weight: torch.Size([1536])\n",
      "transformer.blocks.27.attn.layernorm_qkv.0.bias: torch.Size([1536])\n",
      "transformer.blocks.27.attn.layernorm_qkv.1.weight: torch.Size([4608, 1536])\n",
      "transformer.blocks.27.attn.out_proj.weight: torch.Size([1536, 1536])\n",
      "transformer.blocks.27.attn.q_ln.weight: torch.Size([1536])\n",
      "transformer.blocks.27.attn.k_ln.weight: torch.Size([1536])\n",
      "transformer.blocks.27.ffn.0.weight: torch.Size([1536])\n",
      "transformer.blocks.27.ffn.0.bias: torch.Size([1536])\n",
      "transformer.blocks.27.ffn.1.weight: torch.Size([8192, 1536])\n",
      "transformer.blocks.27.ffn.3.weight: torch.Size([1536, 4096])\n",
      "transformer.blocks.28.attn.layernorm_qkv.0.weight: torch.Size([1536])\n",
      "transformer.blocks.28.attn.layernorm_qkv.0.bias: torch.Size([1536])\n",
      "transformer.blocks.28.attn.layernorm_qkv.1.weight: torch.Size([4608, 1536])\n",
      "transformer.blocks.28.attn.out_proj.weight: torch.Size([1536, 1536])\n",
      "transformer.blocks.28.attn.q_ln.weight: torch.Size([1536])\n",
      "transformer.blocks.28.attn.k_ln.weight: torch.Size([1536])\n",
      "transformer.blocks.28.ffn.0.weight: torch.Size([1536])\n",
      "transformer.blocks.28.ffn.0.bias: torch.Size([1536])\n",
      "transformer.blocks.28.ffn.1.weight: torch.Size([8192, 1536])\n",
      "transformer.blocks.28.ffn.3.weight: torch.Size([1536, 4096])\n",
      "transformer.blocks.29.attn.layernorm_qkv.0.weight: torch.Size([1536])\n",
      "transformer.blocks.29.attn.layernorm_qkv.0.bias: torch.Size([1536])\n",
      "transformer.blocks.29.attn.layernorm_qkv.1.weight: torch.Size([4608, 1536])\n",
      "transformer.blocks.29.attn.out_proj.weight: torch.Size([1536, 1536])\n",
      "transformer.blocks.29.attn.q_ln.weight: torch.Size([1536])\n",
      "transformer.blocks.29.attn.k_ln.weight: torch.Size([1536])\n",
      "transformer.blocks.29.ffn.0.weight: torch.Size([1536])\n",
      "transformer.blocks.29.ffn.0.bias: torch.Size([1536])\n",
      "transformer.blocks.29.ffn.1.weight: torch.Size([8192, 1536])\n",
      "transformer.blocks.29.ffn.3.weight: torch.Size([1536, 4096])\n",
      "transformer.blocks.30.attn.layernorm_qkv.0.weight: torch.Size([1536])\n",
      "transformer.blocks.30.attn.layernorm_qkv.0.bias: torch.Size([1536])\n",
      "transformer.blocks.30.attn.layernorm_qkv.1.weight: torch.Size([4608, 1536])\n",
      "transformer.blocks.30.attn.out_proj.weight: torch.Size([1536, 1536])\n",
      "transformer.blocks.30.attn.q_ln.weight: torch.Size([1536])\n",
      "transformer.blocks.30.attn.k_ln.weight: torch.Size([1536])\n",
      "transformer.blocks.30.ffn.0.weight: torch.Size([1536])\n",
      "transformer.blocks.30.ffn.0.bias: torch.Size([1536])\n",
      "transformer.blocks.30.ffn.1.weight: torch.Size([8192, 1536])\n",
      "transformer.blocks.30.ffn.3.weight: torch.Size([1536, 4096])\n",
      "transformer.blocks.31.attn.layernorm_qkv.0.weight: torch.Size([1536])\n",
      "transformer.blocks.31.attn.layernorm_qkv.0.bias: torch.Size([1536])\n",
      "transformer.blocks.31.attn.layernorm_qkv.1.weight: torch.Size([4608, 1536])\n",
      "transformer.blocks.31.attn.out_proj.weight: torch.Size([1536, 1536])\n",
      "transformer.blocks.31.attn.q_ln.weight: torch.Size([1536])\n",
      "transformer.blocks.31.attn.k_ln.weight: torch.Size([1536])\n",
      "transformer.blocks.31.ffn.0.weight: torch.Size([1536])\n",
      "transformer.blocks.31.ffn.0.bias: torch.Size([1536])\n",
      "transformer.blocks.31.ffn.1.weight: torch.Size([8192, 1536])\n",
      "transformer.blocks.31.ffn.3.weight: torch.Size([1536, 4096])\n",
      "transformer.blocks.32.attn.layernorm_qkv.0.weight: torch.Size([1536])\n",
      "transformer.blocks.32.attn.layernorm_qkv.0.bias: torch.Size([1536])\n",
      "transformer.blocks.32.attn.layernorm_qkv.1.weight: torch.Size([4608, 1536])\n",
      "transformer.blocks.32.attn.out_proj.weight: torch.Size([1536, 1536])\n",
      "transformer.blocks.32.attn.q_ln.weight: torch.Size([1536])\n",
      "transformer.blocks.32.attn.k_ln.weight: torch.Size([1536])\n",
      "transformer.blocks.32.ffn.0.weight: torch.Size([1536])\n",
      "transformer.blocks.32.ffn.0.bias: torch.Size([1536])\n",
      "transformer.blocks.32.ffn.1.weight: torch.Size([8192, 1536])\n",
      "transformer.blocks.32.ffn.3.weight: torch.Size([1536, 4096])\n",
      "transformer.blocks.33.attn.layernorm_qkv.0.weight: torch.Size([1536])\n",
      "transformer.blocks.33.attn.layernorm_qkv.0.bias: torch.Size([1536])\n",
      "transformer.blocks.33.attn.layernorm_qkv.1.weight: torch.Size([4608, 1536])\n",
      "transformer.blocks.33.attn.out_proj.weight: torch.Size([1536, 1536])\n",
      "transformer.blocks.33.attn.q_ln.weight: torch.Size([1536])\n",
      "transformer.blocks.33.attn.k_ln.weight: torch.Size([1536])\n",
      "transformer.blocks.33.ffn.0.weight: torch.Size([1536])\n",
      "transformer.blocks.33.ffn.0.bias: torch.Size([1536])\n",
      "transformer.blocks.33.ffn.1.weight: torch.Size([8192, 1536])\n",
      "transformer.blocks.33.ffn.3.weight: torch.Size([1536, 4096])\n",
      "transformer.blocks.34.attn.layernorm_qkv.0.weight: torch.Size([1536])\n",
      "transformer.blocks.34.attn.layernorm_qkv.0.bias: torch.Size([1536])\n",
      "transformer.blocks.34.attn.layernorm_qkv.1.weight: torch.Size([4608, 1536])\n",
      "transformer.blocks.34.attn.out_proj.weight: torch.Size([1536, 1536])\n",
      "transformer.blocks.34.attn.q_ln.weight: torch.Size([1536])\n",
      "transformer.blocks.34.attn.k_ln.weight: torch.Size([1536])\n",
      "transformer.blocks.34.ffn.0.weight: torch.Size([1536])\n",
      "transformer.blocks.34.ffn.0.bias: torch.Size([1536])\n",
      "transformer.blocks.34.ffn.1.weight: torch.Size([8192, 1536])\n",
      "transformer.blocks.34.ffn.3.weight: torch.Size([1536, 4096])\n",
      "transformer.blocks.35.attn.layernorm_qkv.0.weight: torch.Size([1536])\n",
      "transformer.blocks.35.attn.layernorm_qkv.0.bias: torch.Size([1536])\n",
      "transformer.blocks.35.attn.layernorm_qkv.1.weight: torch.Size([4608, 1536])\n",
      "transformer.blocks.35.attn.out_proj.weight: torch.Size([1536, 1536])\n",
      "transformer.blocks.35.attn.q_ln.weight: torch.Size([1536])\n",
      "transformer.blocks.35.attn.k_ln.weight: torch.Size([1536])\n",
      "transformer.blocks.35.ffn.0.weight: torch.Size([1536])\n",
      "transformer.blocks.35.ffn.0.bias: torch.Size([1536])\n",
      "transformer.blocks.35.ffn.1.weight: torch.Size([8192, 1536])\n",
      "transformer.blocks.35.ffn.3.weight: torch.Size([1536, 4096])\n",
      "transformer.blocks.36.attn.layernorm_qkv.0.weight: torch.Size([1536])\n",
      "transformer.blocks.36.attn.layernorm_qkv.0.bias: torch.Size([1536])\n",
      "transformer.blocks.36.attn.layernorm_qkv.1.weight: torch.Size([4608, 1536])\n",
      "transformer.blocks.36.attn.out_proj.weight: torch.Size([1536, 1536])\n",
      "transformer.blocks.36.attn.q_ln.weight: torch.Size([1536])\n",
      "transformer.blocks.36.attn.k_ln.weight: torch.Size([1536])\n",
      "transformer.blocks.36.ffn.0.weight: torch.Size([1536])\n",
      "transformer.blocks.36.ffn.0.bias: torch.Size([1536])\n",
      "transformer.blocks.36.ffn.1.weight: torch.Size([8192, 1536])\n",
      "transformer.blocks.36.ffn.3.weight: torch.Size([1536, 4096])\n",
      "transformer.blocks.37.attn.layernorm_qkv.0.weight: torch.Size([1536])\n",
      "transformer.blocks.37.attn.layernorm_qkv.0.bias: torch.Size([1536])\n",
      "transformer.blocks.37.attn.layernorm_qkv.1.weight: torch.Size([4608, 1536])\n",
      "transformer.blocks.37.attn.out_proj.weight: torch.Size([1536, 1536])\n",
      "transformer.blocks.37.attn.q_ln.weight: torch.Size([1536])\n",
      "transformer.blocks.37.attn.k_ln.weight: torch.Size([1536])\n",
      "transformer.blocks.37.ffn.0.weight: torch.Size([1536])\n",
      "transformer.blocks.37.ffn.0.bias: torch.Size([1536])\n",
      "transformer.blocks.37.ffn.1.weight: torch.Size([8192, 1536])\n",
      "transformer.blocks.37.ffn.3.weight: torch.Size([1536, 4096])\n",
      "transformer.blocks.38.attn.layernorm_qkv.0.weight: torch.Size([1536])\n",
      "transformer.blocks.38.attn.layernorm_qkv.0.bias: torch.Size([1536])\n",
      "transformer.blocks.38.attn.layernorm_qkv.1.weight: torch.Size([4608, 1536])\n",
      "transformer.blocks.38.attn.out_proj.weight: torch.Size([1536, 1536])\n",
      "transformer.blocks.38.attn.q_ln.weight: torch.Size([1536])\n",
      "transformer.blocks.38.attn.k_ln.weight: torch.Size([1536])\n",
      "transformer.blocks.38.ffn.0.weight: torch.Size([1536])\n",
      "transformer.blocks.38.ffn.0.bias: torch.Size([1536])\n",
      "transformer.blocks.38.ffn.1.weight: torch.Size([8192, 1536])\n",
      "transformer.blocks.38.ffn.3.weight: torch.Size([1536, 4096])\n",
      "transformer.blocks.39.attn.layernorm_qkv.0.weight: torch.Size([1536])\n",
      "transformer.blocks.39.attn.layernorm_qkv.0.bias: torch.Size([1536])\n",
      "transformer.blocks.39.attn.layernorm_qkv.1.weight: torch.Size([4608, 1536])\n",
      "transformer.blocks.39.attn.out_proj.weight: torch.Size([1536, 1536])\n",
      "transformer.blocks.39.attn.q_ln.weight: torch.Size([1536])\n",
      "transformer.blocks.39.attn.k_ln.weight: torch.Size([1536])\n",
      "transformer.blocks.39.ffn.0.weight: torch.Size([1536])\n",
      "transformer.blocks.39.ffn.0.bias: torch.Size([1536])\n",
      "transformer.blocks.39.ffn.1.weight: torch.Size([8192, 1536])\n",
      "transformer.blocks.39.ffn.3.weight: torch.Size([1536, 4096])\n",
      "transformer.blocks.40.attn.layernorm_qkv.0.weight: torch.Size([1536])\n",
      "transformer.blocks.40.attn.layernorm_qkv.0.bias: torch.Size([1536])\n",
      "transformer.blocks.40.attn.layernorm_qkv.1.weight: torch.Size([4608, 1536])\n",
      "transformer.blocks.40.attn.out_proj.weight: torch.Size([1536, 1536])\n",
      "transformer.blocks.40.attn.q_ln.weight: torch.Size([1536])\n",
      "transformer.blocks.40.attn.k_ln.weight: torch.Size([1536])\n",
      "transformer.blocks.40.ffn.0.weight: torch.Size([1536])\n",
      "transformer.blocks.40.ffn.0.bias: torch.Size([1536])\n",
      "transformer.blocks.40.ffn.1.weight: torch.Size([8192, 1536])\n",
      "transformer.blocks.40.ffn.3.weight: torch.Size([1536, 4096])\n",
      "transformer.blocks.41.attn.layernorm_qkv.0.weight: torch.Size([1536])\n",
      "transformer.blocks.41.attn.layernorm_qkv.0.bias: torch.Size([1536])\n",
      "transformer.blocks.41.attn.layernorm_qkv.1.weight: torch.Size([4608, 1536])\n",
      "transformer.blocks.41.attn.out_proj.weight: torch.Size([1536, 1536])\n",
      "transformer.blocks.41.attn.q_ln.weight: torch.Size([1536])\n",
      "transformer.blocks.41.attn.k_ln.weight: torch.Size([1536])\n",
      "transformer.blocks.41.ffn.0.weight: torch.Size([1536])\n",
      "transformer.blocks.41.ffn.0.bias: torch.Size([1536])\n",
      "transformer.blocks.41.ffn.1.weight: torch.Size([8192, 1536])\n",
      "transformer.blocks.41.ffn.3.weight: torch.Size([1536, 4096])\n",
      "transformer.blocks.42.attn.layernorm_qkv.0.weight: torch.Size([1536])\n",
      "transformer.blocks.42.attn.layernorm_qkv.0.bias: torch.Size([1536])\n",
      "transformer.blocks.42.attn.layernorm_qkv.1.weight: torch.Size([4608, 1536])\n",
      "transformer.blocks.42.attn.out_proj.weight: torch.Size([1536, 1536])\n",
      "transformer.blocks.42.attn.q_ln.weight: torch.Size([1536])\n",
      "transformer.blocks.42.attn.k_ln.weight: torch.Size([1536])\n",
      "transformer.blocks.42.ffn.0.weight: torch.Size([1536])\n",
      "transformer.blocks.42.ffn.0.bias: torch.Size([1536])\n",
      "transformer.blocks.42.ffn.1.weight: torch.Size([8192, 1536])\n",
      "transformer.blocks.42.ffn.3.weight: torch.Size([1536, 4096])\n",
      "transformer.blocks.43.attn.layernorm_qkv.0.weight: torch.Size([1536])\n",
      "transformer.blocks.43.attn.layernorm_qkv.0.bias: torch.Size([1536])\n",
      "transformer.blocks.43.attn.layernorm_qkv.1.weight: torch.Size([4608, 1536])\n",
      "transformer.blocks.43.attn.out_proj.weight: torch.Size([1536, 1536])\n",
      "transformer.blocks.43.attn.q_ln.weight: torch.Size([1536])\n",
      "transformer.blocks.43.attn.k_ln.weight: torch.Size([1536])\n",
      "transformer.blocks.43.ffn.0.weight: torch.Size([1536])\n",
      "transformer.blocks.43.ffn.0.bias: torch.Size([1536])\n",
      "transformer.blocks.43.ffn.1.weight: torch.Size([8192, 1536])\n",
      "transformer.blocks.43.ffn.3.weight: torch.Size([1536, 4096])\n",
      "transformer.blocks.44.attn.layernorm_qkv.0.weight: torch.Size([1536])\n",
      "transformer.blocks.44.attn.layernorm_qkv.0.bias: torch.Size([1536])\n",
      "transformer.blocks.44.attn.layernorm_qkv.1.weight: torch.Size([4608, 1536])\n",
      "transformer.blocks.44.attn.out_proj.weight: torch.Size([1536, 1536])\n",
      "transformer.blocks.44.attn.q_ln.weight: torch.Size([1536])\n",
      "transformer.blocks.44.attn.k_ln.weight: torch.Size([1536])\n",
      "transformer.blocks.44.ffn.0.weight: torch.Size([1536])\n",
      "transformer.blocks.44.ffn.0.bias: torch.Size([1536])\n",
      "transformer.blocks.44.ffn.1.weight: torch.Size([8192, 1536])\n",
      "transformer.blocks.44.ffn.3.weight: torch.Size([1536, 4096])\n",
      "transformer.blocks.45.attn.layernorm_qkv.0.weight: torch.Size([1536])\n",
      "transformer.blocks.45.attn.layernorm_qkv.0.bias: torch.Size([1536])\n",
      "transformer.blocks.45.attn.layernorm_qkv.1.weight: torch.Size([4608, 1536])\n",
      "transformer.blocks.45.attn.out_proj.weight: torch.Size([1536, 1536])\n",
      "transformer.blocks.45.attn.q_ln.weight: torch.Size([1536])\n",
      "transformer.blocks.45.attn.k_ln.weight: torch.Size([1536])\n",
      "transformer.blocks.45.ffn.0.weight: torch.Size([1536])\n",
      "transformer.blocks.45.ffn.0.bias: torch.Size([1536])\n",
      "transformer.blocks.45.ffn.1.weight: torch.Size([8192, 1536])\n",
      "transformer.blocks.45.ffn.3.weight: torch.Size([1536, 4096])\n",
      "transformer.blocks.46.attn.layernorm_qkv.0.weight: torch.Size([1536])\n",
      "transformer.blocks.46.attn.layernorm_qkv.0.bias: torch.Size([1536])\n",
      "transformer.blocks.46.attn.layernorm_qkv.1.weight: torch.Size([4608, 1536])\n",
      "transformer.blocks.46.attn.out_proj.weight: torch.Size([1536, 1536])\n",
      "transformer.blocks.46.attn.q_ln.weight: torch.Size([1536])\n",
      "transformer.blocks.46.attn.k_ln.weight: torch.Size([1536])\n",
      "transformer.blocks.46.ffn.0.weight: torch.Size([1536])\n",
      "transformer.blocks.46.ffn.0.bias: torch.Size([1536])\n",
      "transformer.blocks.46.ffn.1.weight: torch.Size([8192, 1536])\n",
      "transformer.blocks.46.ffn.3.weight: torch.Size([1536, 4096])\n",
      "transformer.blocks.47.attn.layernorm_qkv.0.weight: torch.Size([1536])\n",
      "transformer.blocks.47.attn.layernorm_qkv.0.bias: torch.Size([1536])\n",
      "transformer.blocks.47.attn.layernorm_qkv.1.weight: torch.Size([4608, 1536])\n",
      "transformer.blocks.47.attn.out_proj.weight: torch.Size([1536, 1536])\n",
      "transformer.blocks.47.attn.q_ln.weight: torch.Size([1536])\n",
      "transformer.blocks.47.attn.k_ln.weight: torch.Size([1536])\n",
      "transformer.blocks.47.ffn.0.weight: torch.Size([1536])\n",
      "transformer.blocks.47.ffn.0.bias: torch.Size([1536])\n",
      "transformer.blocks.47.ffn.1.weight: torch.Size([8192, 1536])\n",
      "transformer.blocks.47.ffn.3.weight: torch.Size([1536, 4096])\n",
      "transformer.norm.weight: torch.Size([1536])\n",
      "Total number of parameters: 1375643648\n"
     ]
    }
   ],
   "source": [
    "state_dict = loaded_backbone_model.state_dict()\n",
    "for key, tensor in state_dict.items():\n",
    "    print(f\"{key}: {tensor.shape}\")\n",
    "    \n",
    "total_params = sum(p.numel() for p in loaded_backbone_model.parameters())\n",
    "print(\"Total number of parameters:\", total_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "579ab006-ede5-4890-ae35-a52a2bca6f60",
   "metadata": {},
   "source": [
    "# Validate encoder is working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "21256c93-060e-4f20-a7dc-c73cbd0d4709",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from esm.tokenization.sequence_tokenizer import EsmSequenceTokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "cc474281-bc1f-4dc0-9216-18b22589b582",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings shape: torch.Size([1, 17, 1536])\n"
     ]
    }
   ],
   "source": [
    "# for ESM3\n",
    "# Initialize sequence \n",
    "seq_tokenizer = EsmSequenceTokenizer()\n",
    "test_sequence = \"CSSDGSYGFGAMDYW\"\n",
    "seq_tokens = seq_tokenizer.encode(test_sequence)\n",
    "seq_tokens_tensor = torch.tensor(seq_tokens, dtype=torch.int64).unsqueeze(0).to(device)\n",
    "\n",
    "# Create required tensors\n",
    "# dummy_average_plddt = torch.ones(seq_tokens_tensor.shape, dtype=torch.float32, device=device)\n",
    "# dummy_per_res_plddt = torch.ones(seq_tokens_tensor.shape, dtype=torch.float32, device=device)\n",
    "# dummy_structure_tokens = torch.zeros(seq_tokens_tensor.shape, dtype=torch.int64, device=device)\n",
    "\n",
    "# Run forward pass\n",
    "with torch.no_grad():\n",
    "    embeddings = loaded_backbone_model.encoder.sequence_embed(seq_tokens_tensor)\n",
    "    print(\"Embeddings shape:\", embeddings.shape)\n",
    "    \n",
    "    \n",
    "# validate other embedding types work (structure, etc. for ESM3)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0dbb91db-d0d7-47ce-8b4b-e1ab24e2ef7d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EncodeInputs(\n",
       "  (sequence_embed): Embedding(64, 1536)\n",
       "  (plddt_projection): Linear(in_features=16, out_features=1536, bias=True)\n",
       "  (structure_per_res_plddt_projection): Linear(in_features=16, out_features=1536, bias=True)\n",
       "  (structure_tokens_embed): Embedding(4101, 1536)\n",
       "  (ss8_embed): Embedding(11, 1536)\n",
       "  (sasa_embed): Embedding(19, 1536)\n",
       "  (function_embed): ModuleList(\n",
       "    (0-7): 8 x Embedding(260, 192, padding_idx=0)\n",
       "  )\n",
       "  (residue_embed): EmbeddingBag(1478, 1536, mode='sum', padding_idx=0)\n",
       ")"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_backbone_model.encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "d93084d4-7925-4639-b420-3c0f84752ec6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from esm.tokenization.sequence_tokenizer import EsmSequenceTokenizer\n",
    "from esm.models.esm3 import ESM3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "05ea9f87-e71a-48ed-87a2-859e8caf6617",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "3a09dbaf-035c-44a0-b5c3-f14b93bf75e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_tokenizer = EsmSequenceTokenizer()\n",
    "test_sequence = \"CSSDGSYGFGAMDYW\"\n",
    "seq_tokens = seq_tokenizer.encode(test_sequence)\n",
    "seq_tokens_tensor = torch.tensor(seq_tokens, dtype=torch.int64).unsqueeze(0).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "0224a21a-37c7-496f-a19e-e8892bb666bb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dummy_structure_tokens = torch.zeros_like(seq_tokens_tensor, dtype=torch.int64, device=device)\n",
    "\n",
    "# Dummy tokens for ss8 (secondary structure, 11 classes) and SASA (solvent accessibility, 19 classes)\n",
    "dummy_ss8_tokens = torch.zeros_like(seq_tokens_tensor, dtype=torch.int64, device=device)\n",
    "dummy_sasa_tokens = torch.zeros_like(seq_tokens_tensor, dtype=torch.int64, device=device)\n",
    "\n",
    "# Dummy pLDDT values (typically float values per residue)\n",
    "dummy_average_plddt = torch.ones_like(seq_tokens_tensor, dtype=torch.float32, device=device)\n",
    "dummy_per_res_plddt = torch.ones_like(seq_tokens_tensor, dtype=torch.float32, device=device)\n",
    "\n",
    "batch_size, seq_len = seq_tokens_tensor.shape\n",
    "dummy_rbf = torch.ones(batch_size, seq_len, 16,\n",
    "                       dtype=loaded_backbone_model.encoder.plddt_projection.weight.dtype,\n",
    "                       device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "39cc6ec3-0719-44c0-a003-1958809d2255",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], device='cuda:0')"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy_structure_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "eea073f9-a915-4439-8017-ce39d31d449e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 17, 1536])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "structure_embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2593aa5-42bf-4be7-b295-e4744aad65bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "2af5f074-9c2e-4f4c-8b9c-466768be9fea",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence embedding shape: torch.Size([1, 17, 1536])\n",
      "Structure tokens embedding shape: torch.Size([1, 17, 1536])\n",
      "pLDDT projection output shape: torch.Size([1, 17, 1536])\n",
      "Structure per-res pLDDT projection output shape: torch.Size([1, 17, 1536])\n",
      "SS8 embedding shape: torch.Size([1, 17, 1536])\n",
      "SASA embedding shape: torch.Size([1, 17, 1536])\n",
      "Function embedding 0 shape: torch.Size([1, 17, 192])\n",
      "Function embedding 1 shape: torch.Size([1, 17, 192])\n",
      "Function embedding 2 shape: torch.Size([1, 17, 192])\n",
      "Function embedding 3 shape: torch.Size([1, 17, 192])\n",
      "Function embedding 4 shape: torch.Size([1, 17, 192])\n",
      "Function embedding 5 shape: torch.Size([1, 17, 192])\n",
      "Function embedding 6 shape: torch.Size([1, 17, 192])\n",
      "Function embedding 7 shape: torch.Size([1, 17, 192])\n",
      "Residue embedding shape: torch.Size([1, 1536])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    # 1. Sequence embedding:\n",
    "    seq_embedding = loaded_backbone_model.encoder.sequence_embed(seq_tokens_tensor)\n",
    "    print(\"Sequence embedding shape:\", seq_embedding.shape)\n",
    "    \n",
    "    # 2. Structure tokens embedding:\n",
    "    structure_embedding = loaded_backbone_model.encoder.structure_tokens_embed(dummy_structure_tokens)\n",
    "    print(\"Structure tokens embedding shape:\", structure_embedding.shape)\n",
    "    \n",
    "    # 3. pLDDT projection:\n",
    "    plddt_embedding = loaded_backbone_model.encoder.plddt_projection(dummy_rbf)\n",
    "    print(\"pLDDT projection output shape:\", plddt_embedding.shape)\n",
    "    \n",
    "    # 4. Structure per-res pLDDT projection:\n",
    "    structure_plddt_embedding = loaded_backbone_model.encoder.structure_per_res_plddt_projection(dummy_rbf)\n",
    "    print(\"Structure per-res pLDDT projection output shape:\", structure_plddt_embedding.shape)\n",
    "    \n",
    "    # 5. SS8 embedding:\n",
    "    ss8_embedding = loaded_backbone_model.encoder.ss8_embed(dummy_ss8_tokens)\n",
    "    print(\"SS8 embedding shape:\", ss8_embedding.shape)\n",
    "    \n",
    "    # 6. SASA embedding:\n",
    "    sasa_embedding = loaded_backbone_model.encoder.sasa_embed(dummy_sasa_tokens)\n",
    "    print(\"SASA embedding shape:\", sasa_embedding.shape)\n",
    "    \n",
    "    # 7. Function embeddings (a ModuleList of 8 embeddings, each is an independent layer):\n",
    "    for i, func_embed in enumerate(loaded_backbone_model.encoder.function_embed):\n",
    "        # Here we simply use the same dummy sequence tokens for input.\n",
    "        func_embedding = func_embed(seq_tokens_tensor)\n",
    "        print(f\"Function embedding {i} shape:\", func_embedding.shape)\n",
    "    \n",
    "    # 8. Residue annotation embedding using EmbeddingBag:\n",
    "    # EmbeddingBag expects a 1D tensor of indices and an offsets tensor.\n",
    "    dummy_residue_tokens = torch.zeros(seq_tokens_tensor.shape, dtype=torch.int64, device=device)\n",
    "    dummy_residue_tokens_flat = dummy_residue_tokens.view(-1)\n",
    "    offsets = torch.tensor([0], dtype=torch.int64, device=device)\n",
    "    residue_embedding = loaded_backbone_model.encoder.residue_embed(dummy_residue_tokens_flat, offsets)\n",
    "    print(\"Residue embedding shape:\", residue_embedding.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "9790887a-73e1-4d87-9647-0a201250d4c9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], device='cuda:0')"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy_sasa_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "2c523b72-d0a8-49d4-8c2c-8506741ad1ba",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dummy_sasa_tokens[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "729e0f1e-b530-4aea-9d6e-cd732faaf85b",
   "metadata": {
    "tags": []
   },
   "source": [
    "# ESM3 Use PDB File"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f1265d6-5a8d-4fae-9153-362302b4eff1",
   "metadata": {},
   "source": [
    "# Structure Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "820e38e8-44aa-4f1a-90e1-51b6d084c26f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Structure tokens shape: torch.Size([214, 214])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from Bio.PDB import *\n",
    "from scipy.spatial.transform import Rotation\n",
    "\n",
    "def get_backbone_frames(coords):\n",
    "    \"\"\"Get local backbone frames from atomic coordinates\"\"\"\n",
    "    n_atoms = coords.shape[0]\n",
    "    \n",
    "    # Calculate local frame using N, CA, C atoms\n",
    "    n_ca = coords[:, 1] - coords[:, 0]  # N->CA vectors\n",
    "    ca_c = coords[:, 2] - coords[:, 1]  # CA->C vectors\n",
    "    \n",
    "    # Normalize vectors\n",
    "    n_ca = n_ca / torch.norm(n_ca, dim=-1, keepdim=True)\n",
    "    ca_c = ca_c / torch.norm(ca_c, dim=-1, keepdim=True)\n",
    "    \n",
    "    # Calculate frame axes\n",
    "    t = ca_c\n",
    "    n = torch.cross(n_ca, ca_c)\n",
    "    n = n / torch.norm(n, dim=-1, keepdim=True)\n",
    "    b = torch.cross(t, n)\n",
    "    \n",
    "    # Stack into rotation matrices\n",
    "    frames = torch.stack([n, b, t], dim=-1)\n",
    "    return frames\n",
    "\n",
    "def get_relative_orientations(frames):\n",
    "    \"\"\"Get relative orientations between backbone frames\"\"\"\n",
    "    n_frames = frames.shape[0]\n",
    "    \n",
    "    # Calculate relative rotations between all pairs\n",
    "    rel_rots = torch.matmul(frames.unsqueeze(1), frames.transpose(-2, -1).unsqueeze(0))\n",
    "    \n",
    "    # Convert to quaternions\n",
    "    rel_quats = torch.zeros((n_frames, n_frames, 4), device=frames.device)\n",
    "    for i in range(n_frames):\n",
    "        for j in range(n_frames):\n",
    "            r = Rotation.from_matrix(rel_rots[i,j].cpu().numpy())\n",
    "            rel_quats[i,j] = torch.tensor(r.as_quat(), device=frames.device)\n",
    "            \n",
    "    return rel_quats\n",
    "\n",
    "def structure_to_tokens(coords, max_radius=20.0, n_bins=50):\n",
    "    \"\"\"Convert structure to ESM tokens\"\"\"\n",
    "    # Get CA distances\n",
    "    ca_coords = coords[:, 1]  # CA atoms\n",
    "    dists = torch.cdist(ca_coords, ca_coords)\n",
    "    \n",
    "    # Get orientations\n",
    "    frames = get_backbone_frames(coords)\n",
    "    orientations = get_relative_orientations(frames)\n",
    "    \n",
    "    # Discretize distances\n",
    "    dist_bins = torch.linspace(0, max_radius, n_bins, device=coords.device)\n",
    "    dist_tokens = torch.bucketize(dists.flatten(), dist_bins).reshape(dists.shape)\n",
    "    \n",
    "    # Discretize orientations using quaternion bins\n",
    "    quat_bins = 8  # Number of bins per quaternion component\n",
    "    quat_tokens = torch.floor((orientations + 1) * quat_bins/2).long()\n",
    "    quat_tokens = quat_tokens[..., 0] * quat_bins**3 + \\\n",
    "                  quat_tokens[..., 1] * quat_bins**2 + \\\n",
    "                  quat_tokens[..., 2] * quat_bins + \\\n",
    "                  quat_tokens[..., 3]\n",
    "                  \n",
    "    # Combine distance and orientation tokens\n",
    "    structure_tokens = dist_tokens * quat_bins**4 + quat_tokens\n",
    "    \n",
    "    return structure_tokens\n",
    "\n",
    "# Load and process structure\n",
    "parser = MMCIFParser()\n",
    "with gzip.open('/home/jupyter/dan/data/1bey.cif.gz', 'rt') as f:\n",
    "    structure = parser.get_structure('1BEY', f)\n",
    "\n",
    "# Extract backbone coordinatemms\n",
    "coords = []\n",
    "for model in structure:\n",
    "    for chain in model:\n",
    "        backbone = []\n",
    "        for residue in chain:\n",
    "            if all(atom in residue for atom in ['N', 'CA', 'C']):\n",
    "                backbone.append([\n",
    "                    residue['N'].get_coord(),\n",
    "                    residue['CA'].get_coord(),\n",
    "                    residue['C'].get_coord()\n",
    "                ])\n",
    "        coords.append(backbone)\n",
    "\n",
    "coords = torch.tensor(coords[0], dtype=torch.float32).to(device)\n",
    "\n",
    "# Generate structure tokens\n",
    "structure_tokens = structure_to_tokens(coords)\n",
    "print(f\"Structure tokens shape: {structure_tokens.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "416b0071-24a0-43c2-901e-190dbbdd95db",
   "metadata": {
    "tags": []
   },
   "source": [
    "structure_embedding = loaded_backbone_model.encoder.structure_tokens_embed(structure_tokens)\n",
    "print(\"Structure tokens embedding shape:\", structure_embedding.shape)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "268b9f67-f5c4-4b43-a90a-1ecd27aeb6fe",
   "metadata": {},
   "source": [
    "# objectives:\n",
    "# 1. validate other embeddings work (may need antibody pdbs as input)\n",
    "# 2. validate embedding dimensions / shape\n",
    "# 3. Run multiple sequences in one pass?\n",
    "# 4. Speed test 10, 100, 1000 sequences runtime\n",
    "# 5. Training...OPT 3 guide a fine-tuning of this model (on 5 random antibody sequences)\n",
    "# 6. LORA for training econo.y\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "615d1ebe-6438-490b-b6ed-fcb0f5c3213c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e9c9a7-6aa2-4bdb-a3ee-21dced1e13a2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-env-pytorch-pytorch",
   "name": "workbench-notebooks.m123",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m123"
  },
  "kernelspec": {
   "display_name": "PyTorch 1-13 (Local)",
   "language": "python",
   "name": "conda-env-pytorch-pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
