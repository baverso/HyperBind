{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b81e4cdb-0b26-4e31-8412-1f2e6ea173cd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "from esm.models.esm3 import ESM3\n",
    "from esm.sdk.api import ESM3InferenceClient, ESMProtein, GenerationConfig\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "927f2175-9ccb-43be-86d7-10357cb95ae1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2129101f80b44b08b6b33a69aa123445",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97c9f1f588f14bc1b1080cc1ea03e406",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 22 files:   0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ---------------------------\n",
    "# 1. Login and Load the Model\n",
    "# ---------------------------\n",
    "login()  # Log in with your Hugging Face credentials (ensure you have \"Read\" permission)\n",
    "\n",
    "# Download and instantiate the model on GPU (or \"cpu\" if needed)\n",
    "model: ESM3InferenceClient = ESM3.from_pretrained(\"esm3-open\").to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9bd2890f-4cfa-464e-83cc-99fb57092a12",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# 2. Define a Classifier Head\n",
    "# ---------------------------\n",
    "class ESM3Classifier(nn.Module):\n",
    "    def __init__(self, embedding_dim, num_classes, hidden_dim=256, dropout=0.1):\n",
    "        super(ESM3Classifier, self).__init__()\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(embedding_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, embeddings):\n",
    "        # embeddings: (batch_size, embedding_dim)\n",
    "        logits = self.classifier(embeddings)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8ec18c89-4716-4609-85eb-155a07fb00c6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# 3. Generate a Protein Representation\n",
    "# ---------------------------\n",
    "# Example protein sequence (you can replace with your own)\n",
    "prompt = \"EVQLVESGGGLVQPGGSLRLSCAASGFNIKDTYIHWVRQAPGKGLEWVARIYPTNGYTRYADSVKGRFTISADTSKNTAYLQMNSLRAEDTAVYYCSSDGSYGFGAMDYWGQGTLVTVSSGGGGSGGGGSGGGGSDIQMTQSPSSLSASVGDRVTITCRASQDVNTAVAWYQQKPGKAPKLLIYSASFLYSGVPSRFSGSRSGTDFTLTISSLQPEDFATYYCQQYDQTPPTFGQGTKVEIK\"\n",
    "\n",
    "# Create an ESMProtein instance from the sequence.\n",
    "protein = ESMProtein(sequence=prompt)\n",
    "\n",
    "# Configure generation to obtain a representation.\n",
    "# (Assumes that specifying track=\"representation\" yields token-level embeddings.)\n",
    "gen_config = GenerationConfig(track=\"representation\", num_steps=8, temperature=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4f0d5fd6-4626-4df0-bf56-f6e9737ffc7a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GenerationConfig(track='representation', invalid_ids=[], schedule='cosine', strategy='entropy', num_steps=8, temperature=0.7, temperature_annealing=False, top_p=1.0, condition_on_coordinates_only=True)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9740468b-faf3-4a00-a6e2-b9a5613b0e03",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'ESMProteinTensor' object has no attribute 'representation'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Generate the protein representation.\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m protein \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprotein\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgen_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.10/site-packages/esm/models/esm3.py:387\u001b[0m, in \u001b[0;36mESM3.generate\u001b[0;34m(self, input, config)\u001b[0m\n\u001b[1;32m    385\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: ProteinType, config: GenerationConfig) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ProteinType:\n\u001b[1;32m    386\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Wrap around batched generation.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 387\u001b[0m     proteins \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_generate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    388\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(proteins) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m proteins[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.10/site-packages/esm/models/esm3.py:411\u001b[0m, in \u001b[0;36mESM3.batch_generate\u001b[0;34m(self, inputs, configs)\u001b[0m\n\u001b[1;32m    405\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(inputs[i], t), (\n\u001b[1;32m    406\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPrompts must have the same type. Got \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    407\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mt\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01mand\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mtype\u001b[39m(inputs[i])\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    408\u001b[0m     )\n\u001b[1;32m    410\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(inputs[\u001b[38;5;241m0\u001b[39m], ESMProtein):\n\u001b[0;32m--> 411\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43miterative_sampling_raw\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfigs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m    412\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(inputs[\u001b[38;5;241m0\u001b[39m], ESMProteinTensor):\n\u001b[1;32m    413\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m iterative_sampling_tokens(\n\u001b[1;32m    414\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    415\u001b[0m         inputs,  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m    416\u001b[0m         configs,\n\u001b[1;32m    417\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizers,  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m    418\u001b[0m     )\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.10/site-packages/esm/utils/generation.py:110\u001b[0m, in \u001b[0;36miterative_sampling_raw\u001b[0;34m(client, proteins, configs)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21miterative_sampling_raw\u001b[39m(\n\u001b[1;32m    103\u001b[0m     client: ESM3InferenceClient,\n\u001b[1;32m    104\u001b[0m     proteins: \u001b[38;5;28mlist\u001b[39m[ESMProtein],\n\u001b[1;32m    105\u001b[0m     configs: \u001b[38;5;28mlist\u001b[39m[GenerationConfig],\n\u001b[1;32m    106\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m[ESMProtein \u001b[38;5;241m|\u001b[39m ESMProteinError]:\n\u001b[1;32m    107\u001b[0m     \u001b[38;5;66;03m# Keep structure tokens\u001b[39;00m\n\u001b[1;32m    108\u001b[0m     input_tokens \u001b[38;5;241m=\u001b[39m [client\u001b[38;5;241m.\u001b[39mencode(protein) \u001b[38;5;28;01mfor\u001b[39;00m protein \u001b[38;5;129;01min\u001b[39;00m proteins]\n\u001b[0;32m--> 110\u001b[0m     output_tokens_list \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_generate\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_tokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfigs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    112\u001b[0m     raw_proteins: \u001b[38;5;28mlist\u001b[39m[ESMProtein \u001b[38;5;241m|\u001b[39m ESMProteinError] \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    113\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m output_tokens \u001b[38;5;129;01min\u001b[39;00m output_tokens_list:\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.10/site-packages/esm/models/esm3.py:413\u001b[0m, in \u001b[0;36mESM3.batch_generate\u001b[0;34m(self, inputs, configs)\u001b[0m\n\u001b[1;32m    411\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m iterative_sampling_raw(\u001b[38;5;28mself\u001b[39m, inputs, configs)  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m    412\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(inputs[\u001b[38;5;241m0\u001b[39m], ESMProteinTensor):\n\u001b[0;32m--> 413\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43miterative_sampling_tokens\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    414\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    415\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore\u001b[39;49;00m\n\u001b[1;32m    416\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfigs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    417\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenizers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore\u001b[39;49;00m\n\u001b[1;32m    418\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    419\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    420\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput must be an ESMProtein or ESMProteinTensor\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.10/site-packages/esm/utils/generation.py:360\u001b[0m, in \u001b[0;36miterative_sampling_tokens\u001b[0;34m(client, input_tokens, configs, tokenizers)\u001b[0m\n\u001b[1;32m    357\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m protein, config \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sampled_tokens, configs):\n\u001b[1;32m    358\u001b[0m     track \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mtrack\n\u001b[0;32m--> 360\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprotein\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrack\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    361\u001b[0m         \u001b[38;5;66;03m# We need to sample the entire track.\u001b[39;00m\n\u001b[1;32m    362\u001b[0m         num_sampling_steps \u001b[38;5;241m=\u001b[39m _get_non_special_tokens(protein, tokenizers)\n\u001b[1;32m    363\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'ESMProteinTensor' object has no attribute 'representation'"
     ]
    }
   ],
   "source": [
    "# Generate the protein representation.\n",
    "protein = model.generate(protein, gen_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5086861d-9f27-49c6-b02b-d7aa6977a02f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# At this point, assume that the returned protein object has an attribute 'representation'\n",
    "# which is a list or tensor of token embeddings with shape (seq_length, embedding_dim).\n",
    "# (The exact attribute name may differ, so consult the API docs.)\n",
    "# Convert it to a torch tensor and move to the appropriate device.\n",
    "token_representations = torch.tensor(protein.representation).to(\"cuda\")\n",
    "\n",
    "# ---------------------------\n",
    "# 4. Pool the Token Embeddings\n",
    "# ---------------------------\n",
    "# Use mean pooling to obtain a single embedding vector for the entire sequence.\n",
    "# token_representations: (seq_length, embedding_dim) => pooled_embedding: (embedding_dim,)\n",
    "pooled_embedding = token_representations.mean(dim=0)\n",
    "# Add a batch dimension: (1, embedding_dim)\n",
    "pooled_embedding = pooled_embedding.unsqueeze(0)\n",
    "\n",
    "# ---------------------------\n",
    "# 5. Attach and Run the Classifier Head\n",
    "# ---------------------------\n",
    "# Determine embedding dimension from the pooled embedding\n",
    "embedding_dim = pooled_embedding.size(1)\n",
    "num_classes = 2  # Example: binary classification\n",
    "\n",
    "classifier = ESM3Classifier(embedding_dim, num_classes).to(\"cuda\")\n",
    "\n",
    "# Forward pass through the classifier to obtain logits.\n",
    "logits = classifier(pooled_embedding)\n",
    "print(\"Logits:\", logits)"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-env-pytorch-pytorch",
   "name": "workbench-notebooks.m123",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m123"
  },
  "kernelspec": {
   "display_name": "PyTorch 1-13 (Local)",
   "language": "python",
   "name": "conda-env-pytorch-pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
